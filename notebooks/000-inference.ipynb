{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "\n",
    "import av\n",
    "import cv2\n",
    "import diffusers\n",
    "import numpy as np\n",
    "import safetensors.torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from diffusers import FlowMatchEulerDiscreteScheduler, HunyuanVideoPipeline\n",
    "from diffusers.callbacks import MultiPipelineCallbacks, PipelineCallback\n",
    "from diffusers.loaders import HunyuanVideoLoraLoaderMixin\n",
    "from diffusers.models import AutoencoderKLHunyuanVideo, HunyuanVideoTransformer3DModel\n",
    "from diffusers.models.attention import Attention\n",
    "from diffusers.models.embeddings import apply_rotary_emb\n",
    "from diffusers.models.transformers.transformer_hunyuan_video import *\n",
    "from diffusers.models.transformers.transformer_hunyuan_video import HunyuanVideoPatchEmbed, HunyuanVideoTransformer3DModel\n",
    "from diffusers.pipelines.hunyuan_video.pipeline_hunyuan_video import DEFAULT_PROMPT_TEMPLATE, retrieve_timesteps\n",
    "from diffusers.pipelines.hunyuan_video.pipeline_output import HunyuanVideoPipelineOutput\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n",
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from diffusers.utils import export_to_video, is_torch_xla_available, logging, replace_example_docstring\n",
    "from diffusers.utils.state_dict_utils import convert_state_dict_to_diffusers, convert_unet_state_dict_to_peft\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.video_processor import VideoProcessor\n",
    "from peft import LoraConfig, get_peft_model_state_dict, set_peft_model_state_dict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nENVIRONMENT\\n\")\n",
    "print(f\"  Python {sys.version}\\n\")\n",
    "print(f\"  torch.__version__              = {torch.__version__}\")\n",
    "print(f\"  torch.version.cuda             = {torch.version.cuda}\")\n",
    "print(f\"  torch.backends.cudnn.version() = {torch.backends.cudnn.version()}\")\n",
    "print(f\"  diffusers.__version__          = {diffusers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import flash_attn\n",
    "    from flash_attn.flash_attn_interface import _flash_attn_forward, flash_attn_varlen_func\n",
    "except ImportError:\n",
    "    flash_attn, _flash_attn_forward, flash_attn_varlen_func = None, None, None\n",
    "\n",
    "try:\n",
    "    from sageattention import sageattn, sageattn_varlen\n",
    "except ImportError:\n",
    "    sageattn, sageattn_varlen = None, None\n",
    "\n",
    "\n",
    "def get_cu_seqlens(attention_mask):\n",
    "    \"\"\"Calculate cu_seqlens_q, cu_seqlens_kv using text_mask and img_len\n",
    "\n",
    "    Args:\n",
    "        text_mask (torch.Tensor): the mask of text\n",
    "        img_len (int): the length of image\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: the calculated cu_seqlens for flash attention\n",
    "    \"\"\"\n",
    "    batch_size = attention_mask.shape[0]\n",
    "    text_len = attention_mask.sum(dim=-1, dtype=torch.int)  # .flatten()\n",
    "    max_len = attention_mask.shape[-1]\n",
    "\n",
    "    cu_seqlens = torch.zeros([2 * batch_size + 1], dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        s = text_len[i]\n",
    "        s1 = i * max_len + s\n",
    "        s2 = (i + 1) * max_len\n",
    "        cu_seqlens[2 * i + 1] = s1\n",
    "        cu_seqlens[2 * i + 2] = s2\n",
    "\n",
    "    return cu_seqlens\n",
    "\n",
    "\n",
    "class HunyuanVideoFlashAttnProcessor:\n",
    "    def __init__(self, use_flash_attn=True, use_sageattn=False):\n",
    "        self.use_flash_attn = use_flash_attn\n",
    "        self.use_sageattn = use_sageattn\n",
    "        if self.use_flash_attn:\n",
    "            assert flash_attn is not None\n",
    "        if self.use_sageattn:\n",
    "            assert sageattn is not None\n",
    "\n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, image_rotary_emb=None):\n",
    "        if attn.add_q_proj is None and encoder_hidden_states is not None:\n",
    "            hidden_states = torch.cat([hidden_states, encoder_hidden_states], dim=1)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "        key = attn.to_k(hidden_states)\n",
    "        value = attn.to_v(hidden_states)\n",
    "\n",
    "        query = query.unflatten(2, (attn.heads, -1)).transpose(1, 2)\n",
    "        key = key.unflatten(2, (attn.heads, -1)).transpose(1, 2)\n",
    "        value = value.unflatten(2, (attn.heads, -1)).transpose(1, 2)\n",
    "\n",
    "        if attn.norm_q is not None:\n",
    "            query = attn.norm_q(query)\n",
    "        if attn.norm_k is not None:\n",
    "            key = attn.norm_k(key)\n",
    "\n",
    "        if image_rotary_emb is not None:\n",
    "            if attn.add_q_proj is None and encoder_hidden_states is not None:\n",
    "                query = torch.cat([apply_rotary_emb(query[:, :, : -encoder_hidden_states.shape[1]], image_rotary_emb), query[:, :, -encoder_hidden_states.shape[1] :]], dim=2)\n",
    "                key = torch.cat([apply_rotary_emb(key[:, :, : -encoder_hidden_states.shape[1]], image_rotary_emb), key[:, :, -encoder_hidden_states.shape[1] :]], dim=2)\n",
    "            else:\n",
    "                query = apply_rotary_emb(query, image_rotary_emb)\n",
    "                key = apply_rotary_emb(key, image_rotary_emb)\n",
    "\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        img_seq_len = hidden_states.shape[1]\n",
    "        txt_seq_len = 0\n",
    "\n",
    "        if attn.add_q_proj is not None and encoder_hidden_states is not None:\n",
    "            encoder_query = attn.add_q_proj(encoder_hidden_states)\n",
    "            encoder_key = attn.add_k_proj(encoder_hidden_states)\n",
    "            encoder_value = attn.add_v_proj(encoder_hidden_states)\n",
    "\n",
    "            encoder_query = encoder_query.unflatten(2, (attn.heads, -1)).transpose(1, 2)\n",
    "            encoder_key = encoder_key.unflatten(2, (attn.heads, -1)).transpose(1, 2)\n",
    "            encoder_value = encoder_value.unflatten(2, (attn.heads, -1)).transpose(1, 2)\n",
    "\n",
    "            if attn.norm_added_q is not None:\n",
    "                encoder_query = attn.norm_added_q(encoder_query)\n",
    "            if attn.norm_added_k is not None:\n",
    "                encoder_key = attn.norm_added_k(encoder_key)\n",
    "\n",
    "            query = torch.cat([query, encoder_query], dim=2)\n",
    "            key = torch.cat([key, encoder_key], dim=2)\n",
    "            value = torch.cat([value, encoder_value], dim=2)\n",
    "\n",
    "            txt_seq_len = encoder_hidden_states.shape[1]\n",
    "\n",
    "        max_seqlen_q = max_seqlen_kv = img_seq_len + txt_seq_len\n",
    "        cu_seqlens_q = cu_seqlens_kv = get_cu_seqlens(attention_mask)\n",
    "\n",
    "        query = query.transpose(1, 2).reshape(-1, query.shape[1], query.shape[3])\n",
    "        key = key.transpose(1, 2).reshape(-1, key.shape[1], key.shape[3])\n",
    "        value = value.transpose(1, 2).reshape(-1, value.shape[1], value.shape[3])\n",
    "\n",
    "        if self.use_flash_attn:\n",
    "            hidden_states = flash_attn_varlen_func(query, key, value, cu_seqlens_q, cu_seqlens_kv, max_seqlen_q, max_seqlen_kv)\n",
    "        elif self.use_sageattn:\n",
    "            hidden_states = sageattn_varlen(query, key, value, cu_seqlens_q, cu_seqlens_kv, max_seqlen_q, max_seqlen_kv)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Please set use_flash_attn=True or use_sageattn=True\")\n",
    "\n",
    "        hidden_states = hidden_states.reshape(batch_size, max_seqlen_q, -1)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            hidden_states, encoder_hidden_states = (hidden_states[:, : -encoder_hidden_states.shape[1]], hidden_states[:, -encoder_hidden_states.shape[1] :])\n",
    "\n",
    "            if getattr(attn, \"to_out\", None) is not None:\n",
    "                hidden_states = attn.to_out[0](hidden_states)\n",
    "                hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "            if getattr(attn, \"to_add_out\", None) is not None:\n",
    "                encoder_hidden_states = attn.to_add_out(encoder_hidden_states)\n",
    "\n",
    "        return hidden_states, encoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(video, output_path=\"output.mp4\"):\n",
    "    width, height = video[0].size\n",
    "\n",
    "    container = av.open(output_path, mode=\"w\")\n",
    "\n",
    "    # create video stream\n",
    "    codec = \"libx264\"\n",
    "    pixel_format = \"yuv420p\"\n",
    "    stream = container.add_stream(codec, rate=24)\n",
    "    stream.width = width\n",
    "    stream.height = height\n",
    "    stream.pix_fmt = pixel_format\n",
    "    stream.bit_rate = 4000000  # 4Mbit/s\n",
    "\n",
    "    for frame_array in video:\n",
    "        frame = av.VideoFrame.from_image(frame_array)\n",
    "        packets = stream.encode(frame)\n",
    "        for packet in packets:\n",
    "            container.mux(packet)\n",
    "\n",
    "    for packet in stream.encode():\n",
    "        container.mux(packet)\n",
    "\n",
    "    container.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = HunyuanVideoPipeline.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", torch_dtype=torch.bfloat16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.vae.enable_slicing()\n",
    "pipe.vae.enable_tiling()\n",
    "\n",
    "for block in pipe.transformer.transformer_blocks + pipe.transformer.single_transformer_blocks:\n",
    "    block.attn.processor = HunyuanVideoFlashAttnProcessor(use_flash_attn=True, use_sageattn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    initial_input_channels = pipe.transformer.config.in_channels\n",
    "    new_img_in = HunyuanVideoPatchEmbed(\n",
    "        patch_size=(pipe.transformer.config.patch_size_t, pipe.transformer.config.patch_size, pipe.transformer.config.patch_size),\n",
    "        in_chans=pipe.transformer.config.in_channels * 2,\n",
    "        embed_dim=pipe.transformer.config.num_attention_heads * pipe.transformer.config.attention_head_dim,\n",
    "    )\n",
    "    new_img_in = new_img_in.to(pipe.device, dtype=pipe.dtype)\n",
    "    new_img_in.proj.weight.zero_()\n",
    "    new_img_in.proj.weight[:, :initial_input_channels].copy_(pipe.transformer.x_embedder.proj.weight)\n",
    "\n",
    "    if pipe.transformer.x_embedder.proj.bias is not None:\n",
    "        new_img_in.proj.bias.copy_(pipe.transformer.x_embedder.proj.bias)\n",
    "\n",
    "    pipe.transformer.x_embedder = new_img_in\n",
    "    pipe.transformer.x_embedder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_PATH = \"/mnt/data/ayushman/projects/output/i2v_outputs/exp006-resume-from-9500/i2v.sft\"\n",
    "\n",
    "# ------ load lora\n",
    "lora_state_dict = pipe.lora_state_dict(LORA_PATH)\n",
    "transformer_lora_state_dict = {f'{k.replace(\"transformer.\", \"\")}': v for k, v in lora_state_dict.items() if k.startswith(\"transformer.\") and \"lora\" in k}\n",
    "pipe.load_lora_into_transformer(transformer_lora_state_dict, transformer=pipe.transformer, adapter_name=\"i2v\", _pipeline=pipe)\n",
    "pipe.set_adapters([\"i2v\"], adapter_weights=[1.0])\n",
    "pipe.fuse_lora(components=[\"transformer\"], lora_scale=1.0, adapter_names=[\"i2v\"])\n",
    "pipe.unload_lora_weights()\n",
    "\n",
    "# -------- load norm layers\n",
    "NORM_LAYER_PREFIXES = [\"norm_q\", \"norm_k\", \"norm_added_q\", \"norm_added_k\"]\n",
    "transformer_norm_layers_state_dict = {\n",
    "    f'{k.replace(\"transformer.\", \"\")}': v for k, v in lora_state_dict.items() if k.startswith(\"transformer.\") and any(norm_k in k for norm_k in NORM_LAYER_PREFIXES)\n",
    "}\n",
    "if len(transformer_norm_layers_state_dict) == 0:\n",
    "    print(\"[INFO] No normalization layers found in state dict. Skipping loading normalization layers....\")\n",
    "else:\n",
    "    print(\"[INFO] Extracting normalization layers from state dict and loading them into the transformer....\")\n",
    "    for key in list(transformer_norm_layers_state_dict.keys()):\n",
    "        if key.split(\".\")[0] == \"transformer\":\n",
    "            transformer_norm_layers_state_dict[key[len(f\"transformer.\") :]] = transformer_norm_layers_state_dict.pop(key)\n",
    "    transformer_state_dict = pipe.transformer.state_dict()\n",
    "    transformer_keys = set(transformer_state_dict.keys())\n",
    "    state_dict_keys = set(transformer_norm_layers_state_dict.keys())\n",
    "    extra_keys = list(state_dict_keys - transformer_keys)\n",
    "    if extra_keys:\n",
    "        print(f\"Unsupported keys found in state dict when trying to load normalization layers into the transformer. The following keys will be ignored:\\n{extra_keys}.\")\n",
    "    for key in extra_keys:\n",
    "        transformer_norm_layers_state_dict.pop(key)\n",
    "    incompatible_keys = pipe.transformer.load_state_dict(transformer_norm_layers_state_dict, strict=False)\n",
    "    unexpected_keys = getattr(incompatible_keys, \"unexpected_keys\", None)\n",
    "    if any(norm_key in k for k in unexpected_keys for norm_key in NORM_LAYER_PREFIXES):\n",
    "        print(f\"Found {unexpected_keys} as unexpected keys while trying to load norm layers into the transformer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: x / 255.0),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def resize_image_to_bucket(image: Union[Image.Image, np.ndarray], bucket_reso: tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resize the image to the bucket resolution.\n",
    "    \"\"\"\n",
    "    is_pil_image = isinstance(image, Image.Image)\n",
    "    if is_pil_image:\n",
    "        image_width, image_height = image.size\n",
    "    else:\n",
    "        image_height, image_width = image.shape[:2]\n",
    "\n",
    "    if bucket_reso == (image_width, image_height):\n",
    "        return np.array(image) if is_pil_image else image\n",
    "\n",
    "    bucket_width, bucket_height = bucket_reso\n",
    "\n",
    "    scale_width = bucket_width / image_width\n",
    "    scale_height = bucket_height / image_height\n",
    "    scale = max(scale_width, scale_height)\n",
    "    image_width = int(image_width * scale + 0.5)\n",
    "    image_height = int(image_height * scale + 0.5)\n",
    "\n",
    "    if scale > 1:\n",
    "        image = Image.fromarray(image) if not is_pil_image else image\n",
    "        image = image.resize((image_width, image_height), Image.LANCZOS)\n",
    "        image = np.array(image)\n",
    "    else:\n",
    "        image = np.array(image) if is_pil_image else image\n",
    "        image = cv2.resize(image, (image_width, image_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # crop the image to the bucket resolution\n",
    "    crop_left = (image_width - bucket_width) // 2\n",
    "    crop_top = (image_height - bucket_height) // 2\n",
    "    image = image[crop_top : crop_top + bucket_height, crop_left : crop_left + bucket_width]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames, height, width = 77, 960, 544\n",
    "# prompt = 'a woman moving her head'\n",
    "# p1 = \"/mnt/data/ayushman/projects/input/img_samples/dvqdv.png\"\n",
    "# p2 = \"/mnt/data/ayushman/projects/input/img_samples/F11024.png\"\n",
    "\n",
    "n_frames, height, width = 77, 720, 1280\n",
    "prompt = \"A man and a woman sit at a cozy café table in the warm midday light. She wears a vibrant green dress, and he wears a simple black T-shirt. Their table is adorned with fresh salads, small plates of appetizers, and a sparkling beverage in elegant glassware. They chat casually, occasionally glancing at their phones or laughing together. Large windows in the background reveal a bustling street, where people pass by under soft lanterns. The camera begins with a medium-wide shot, capturing the relaxed ambiance and lively chatter of nearby patrons. It then transitions to a closer view, highlighting the woman's bright smile and the man's attentive expression. Their voices blend with the gentle hum of the café, as they enjoy a pleasant lunch and each other's company.\"\n",
    "p1 = \"/mnt/data/ayushman/projects/input/img_samples/009-F1.png\"\n",
    "p2 = \"/mnt/data/ayushman/projects/input/img_samples/009-F2.png\"\n",
    "\n",
    "\n",
    "# n_frames, height, width = 73, 960, 544\n",
    "# prompt = 'a man'\n",
    "# p1 = \"/mnt/data/ayushman/projects/input/img_samples/3.3.PNG\"\n",
    "# p2 = \"/mnt/data/ayushman/projects/input/img_samples/4.1 (1).PNG\"\n",
    "\n",
    "\n",
    "# n_frames, height, width = 73, 544, 960\n",
    "# prompt = 'realistic style, a man walking out of a cave'\n",
    "# p1 = \"/mnt/data/ayushman/projects/input/img_samples/3015505-hd_1920_1080_24fps-Scene-001-01.jpg\"\n",
    "# p2 = \"/mnt/data/ayushman/projects/input/img_samples/3015505-hd_1920_1080_24fps-Scene-001-02.jpg\"\n",
    "\n",
    "# n_frames, height, width = 73, 544, 960\n",
    "# prompt = 'a whimsical milk cartoon dancing'\n",
    "# p2 = \"/mnt/data/ayushman/projects/input/img_samples/158387-816637360_small-Scene-001-03.jpg\"\n",
    "# p1 = \"/mnt/data/ayushman/projects/input/img_samples/158387-816637360_small-Scene-001-01.jpg\"\n",
    "\n",
    "# n_frames, height, width = 73, 544, 960\n",
    "# prompt = 'a whimsical milk cartoon dancing'\n",
    "# p2 = \"/mnt/data/ayushman/projects/input/img_samples/158387-816637360_small-Scene-001-03.jpg\"\n",
    "# p1 = \"/mnt/data/ayushman/projects/input/img_samples/158387-816637360_small-Scene-001-01.jpg\"\n",
    "\n",
    "# n_frames, height, width = 73, 960, 544\n",
    "# prompt = \"a woman\"\n",
    "# p1 = \"/mnt/data/ayushman/projects/input/img_samples/F14866.png\"\n",
    "# p2 = \"/mnt/data/ayushman/projects/input/img_samples/F14920.png\"\n",
    "\n",
    "assert os.path.exists(p1)\n",
    "assert os.path.exists(p2)\n",
    "\n",
    "cond_frame1 = Image.open(p1).convert(\"RGB\")\n",
    "cond_frame2 = Image.open(p2).convert(\"RGB\")\n",
    "# show_images([cond_frame1, cond_frame2], titles=[\"cond_frame1\", \"cond_frame2\"], imsize=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_frame1 = resize_image_to_bucket(cond_frame1, bucket_reso=(width, height))\n",
    "cond_frame2 = resize_image_to_bucket(cond_frame2, bucket_reso=(width, height))\n",
    "\n",
    "cond_video = np.zeros(shape=(n_frames, height, width, 3))\n",
    "cond_video[0], cond_video[-1] = np.array(cond_frame1), np.array(cond_frame2)\n",
    "\n",
    "cond_video = torch.from_numpy(cond_video.copy()).permute(0, 3, 1, 2)\n",
    "cond_video = torch.stack([video_transforms(x) for x in cond_video], dim=0).unsqueeze(0)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    image_or_video = cond_video.to(device=\"cuda\", dtype=pipe.dtype)\n",
    "    image_or_video = image_or_video.permute(0, 2, 1, 3, 4).contiguous()  # [B, F, C, H, W] -> [B, C, F, H, W]\n",
    "    cond_latents = pipe.vae.encode(image_or_video).latent_dist.sample()\n",
    "    cond_latents = cond_latents * pipe.vae.config.scaling_factor\n",
    "    cond_latents = cond_latents.to(dtype=pipe.dtype)\n",
    "    assert not torch.any(torch.isnan(cond_latents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def call_pipe(\n",
    "    pipe,\n",
    "    prompt: Union[str, List[str]] = None,\n",
    "    prompt_2: Union[str, List[str]] = None,\n",
    "    height: int = 720,\n",
    "    width: int = 1280,\n",
    "    num_frames: int = 129,\n",
    "    num_inference_steps: int = 50,\n",
    "    sigmas: List[float] = None,\n",
    "    guidance_scale: float = 6.0,\n",
    "    num_videos_per_prompt: Optional[int] = 1,\n",
    "    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "    latents: Optional[torch.Tensor] = None,\n",
    "    prompt_embeds: Optional[torch.Tensor] = None,\n",
    "    pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "    prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "    output_type: Optional[str] = \"pil\",\n",
    "    return_dict: bool = True,\n",
    "    attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    callback_on_step_end: Optional[Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]] = None,\n",
    "    callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "    prompt_template: Dict[str, Any] = DEFAULT_PROMPT_TEMPLATE,\n",
    "    max_sequence_length: int = 256,\n",
    "    image_latents: Optional[torch.Tensor] = None,\n",
    "):\n",
    "\n",
    "    if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "        callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "\n",
    "    # 1. Check inputs. Raise error if not correct\n",
    "    pipe.check_inputs(\n",
    "        prompt,\n",
    "        prompt_2,\n",
    "        height,\n",
    "        width,\n",
    "        prompt_embeds,\n",
    "        callback_on_step_end_tensor_inputs,\n",
    "        prompt_template,\n",
    "    )\n",
    "\n",
    "    pipe._guidance_scale = guidance_scale\n",
    "    pipe._attention_kwargs = attention_kwargs\n",
    "    pipe._current_timestep = None\n",
    "    pipe._interrupt = False\n",
    "\n",
    "    device = pipe._execution_device\n",
    "\n",
    "    # 2. Define call parameters\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "    # 3. Encode input prompt\n",
    "    prompt_embeds, pooled_prompt_embeds, prompt_attention_mask = pipe.encode_prompt(\n",
    "        prompt=prompt,\n",
    "        prompt_2=prompt_2,\n",
    "        prompt_template=prompt_template,\n",
    "        num_videos_per_prompt=num_videos_per_prompt,\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "        prompt_attention_mask=prompt_attention_mask,\n",
    "        device=device,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "    )\n",
    "\n",
    "    transformer_dtype = pipe.transformer.dtype\n",
    "    prompt_embeds = prompt_embeds.to(transformer_dtype)\n",
    "    prompt_attention_mask = prompt_attention_mask.to(transformer_dtype)\n",
    "    if pooled_prompt_embeds is not None:\n",
    "        pooled_prompt_embeds = pooled_prompt_embeds.to(transformer_dtype)\n",
    "\n",
    "    # 4. Prepare timesteps\n",
    "    sigmas = np.linspace(1.0, 0.0, num_inference_steps + 1)[:-1] if sigmas is None else sigmas\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(\n",
    "        pipe.scheduler,\n",
    "        num_inference_steps,\n",
    "        device,\n",
    "        sigmas=sigmas,\n",
    "    )\n",
    "\n",
    "    # 5. Prepare latent variables\n",
    "    num_channels_latents = pipe.transformer.config.in_channels\n",
    "    num_latent_frames = (num_frames - 1) // pipe.vae_scale_factor_temporal + 1\n",
    "    latents = pipe.prepare_latents(\n",
    "        batch_size * num_videos_per_prompt,\n",
    "        num_channels_latents,\n",
    "        height,\n",
    "        width,\n",
    "        num_latent_frames,\n",
    "        torch.float32,\n",
    "        device,\n",
    "        generator,\n",
    "        latents,\n",
    "    )\n",
    "\n",
    "    # 6. Prepare guidance condition\n",
    "    guidance = torch.tensor([guidance_scale] * latents.shape[0], dtype=transformer_dtype, device=device) * 1000.0\n",
    "\n",
    "    # 7. Denoising loop\n",
    "    num_warmup_steps = len(timesteps) - num_inference_steps * pipe.scheduler.order\n",
    "    pipe._num_timesteps = len(timesteps)\n",
    "\n",
    "    with pipe.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        for i, t in enumerate(timesteps):\n",
    "            if pipe.interrupt:\n",
    "                continue\n",
    "\n",
    "            pipe._current_timestep = t\n",
    "            latent_model_input = latents.to(transformer_dtype)\n",
    "            timestep = t.expand(latents.shape[0]).to(latents.dtype)\n",
    "\n",
    "            noise_pred = pipe.transformer(\n",
    "                hidden_states=torch.cat([latent_model_input, image_latents], dim=1),\n",
    "                timestep=timestep,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                encoder_attention_mask=prompt_attention_mask,\n",
    "                pooled_projections=pooled_prompt_embeds,\n",
    "                guidance=guidance,\n",
    "                attention_kwargs=attention_kwargs,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = pipe.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "            if callback_on_step_end is not None:\n",
    "                callback_kwargs = {}\n",
    "                for k in callback_on_step_end_tensor_inputs:\n",
    "                    callback_kwargs[k] = locals()[k]\n",
    "                callback_outputs = callback_on_step_end(pipe, i, t, callback_kwargs)\n",
    "\n",
    "                latents = callback_outputs.pop(\"latents\", latents)\n",
    "                prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "\n",
    "            # call the callback, if provided\n",
    "            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipe.scheduler.order == 0):\n",
    "                progress_bar.update()\n",
    "    pipe._current_timestep = None\n",
    "\n",
    "    if not output_type == \"latent\":\n",
    "        latents = latents.to(pipe.vae.dtype) / pipe.vae.config.scaling_factor\n",
    "        video = pipe.vae.decode(latents, return_dict=False)[0]\n",
    "        video = pipe.video_processor.postprocess_video(video, output_type=output_type)\n",
    "    else:\n",
    "        video = latents\n",
    "\n",
    "    # Offload all models\n",
    "    pipe.maybe_free_model_hooks()\n",
    "\n",
    "    if not return_dict:\n",
    "        return (video,)\n",
    "\n",
    "    return HunyuanVideoPipelineOutput(frames=video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = call_pipe(\n",
    "    pipe,\n",
    "    prompt=prompt,\n",
    "    num_frames=n_frames,\n",
    "    num_inference_steps=50,\n",
    "    image_latents=cond_latents,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    guidance_scale=6.0,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(0),\n",
    ").frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_flag = datetime.fromtimestamp(time.time()).strftime(\"%Y%m%d_%H%M%S\")\n",
    "fp = f\"/mnt/data/ayushman/projects/output/samples/hv-CL-{height}x{width}x{n_frames}-{time_flag}.mp4\"\n",
    "print(fp)\n",
    "save_video(video, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hunyuan_control_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
